{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Thought Anchor ë¶„ì„ ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ `anchor_vectors_output/` ë””ë ‰í† ë¦¬ì˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ ê²°ê³¼ íŒŒì¼ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('anchor_vectors_output')\n",
    "result_files = sorted(output_dir.glob('example_*.json'))\n",
    "\n",
    "print(f\"ì´ {len(result_files)}ê°œì˜ ê²°ê³¼ íŒŒì¼ ë°œê²¬:\")\n",
    "for f in result_files:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  - {f.name}: {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ê°œë³„ ì˜ˆì œ ìƒì„¸ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_example(json_path):\n",
    "    \"\"\"ë‹¨ì¼ ì˜ˆì œ ë¶„ì„ ë° ì‹œê°í™”\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    example_name = json_path.stem\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“Š {example_name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ì—ëŸ¬ ì²´í¬\n",
    "    if 'error' in data:\n",
    "        print(f\"âŒ Error: {data['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total pairs: {data['total_pairs']}\")\n",
    "    print(f\"Successful pairs: {data['successful_pairs']}\")\n",
    "    print(f\"Image: {data.get('image_path', 'N/A')}\")\n",
    "    print()\n",
    "    \n",
    "    # QA pair ë¶„ì„\n",
    "    for qa_idx, qa_pair in enumerate(data.get('qa_pairs', [])):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"QA Pair {qa_idx + 1}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # ì§ˆë¬¸\n",
    "        question = qa_pair.get('question', '')\n",
    "        if '<|vision_start|>' in question:\n",
    "            question = question.split('\\n')[-1]  # ë§ˆì§€ë§‰ ì¤„ë§Œ\n",
    "        print(f\"â“ Question:\\n   {question}\\n\")\n",
    "        \n",
    "        # Reasoning\n",
    "        reasoning = qa_pair.get('reasoning_text', '')\n",
    "        print(f\"ğŸ’­ Reasoning:\\n{reasoning}\\n\")\n",
    "        \n",
    "        # Chunks\n",
    "        chunks = qa_pair.get('chunks', [])\n",
    "        anchor_vector = np.array(qa_pair.get('anchor_vector', []))\n",
    "        \n",
    "        if len(chunks) == 0:\n",
    "            print(\"âš ï¸  No chunks found.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"ğŸ“ {len(chunks)} Chunks with Anchor Scores:\\n\")\n",
    "        \n",
    "        # Chunkì™€ Anchor Score í•¨ê»˜ í‘œì‹œ\n",
    "        for i, (chunk, score) in enumerate(zip(chunks, anchor_vector)):\n",
    "            # ì¤‘ìš”ë„ì— ë”°ë¼ ìƒ‰ìƒ í‘œì‹œ\n",
    "            if score > 12:\n",
    "                marker = \"ğŸ”´\"  # ë§¤ìš° ì¤‘ìš”\n",
    "            elif score > 10:\n",
    "                marker = \"ğŸŸ \"  # ì¤‘ìš”\n",
    "            elif score > 5:\n",
    "                marker = \"ğŸŸ¡\"  # ë³´í†µ\n",
    "            else:\n",
    "                marker = \"âšª\"  # ë‚®ìŒ\n",
    "            \n",
    "            print(f\"  {marker} Chunk {i} (score: {score:.2f}):\")\n",
    "            print(f\"     {chunk[:120]}...\" if len(chunk) > 120 else f\"     {chunk}\")\n",
    "            print()\n",
    "        \n",
    "        # Anchor Score ì‹œê°í™”\n",
    "        if len(anchor_vector) > 1 and anchor_vector.max() > 0:\n",
    "            fig, ax = plt.subplots(figsize=(10, 4))\n",
    "            \n",
    "            colors = ['red' if s > 12 else 'orange' if s > 10 else 'yellow' if s > 5 else 'lightgray' \n",
    "                      for s in anchor_vector]\n",
    "            \n",
    "            bars = ax.bar(range(len(anchor_vector)), anchor_vector, color=colors, alpha=0.7, edgecolor='black')\n",
    "            ax.set_xlabel('Chunk Index', fontsize=12)\n",
    "            ax.set_ylabel('Anchor Score', fontsize=12)\n",
    "            ax.set_title(f'{example_name} - QA Pair {qa_idx + 1}: Anchor Scores', fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(range(len(anchor_vector)))\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # ê°’ í‘œì‹œ\n",
    "            for i, (bar, val) in enumerate(zip(bars, anchor_vector)):\n",
    "                if val > 0.1:\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, val + 0.5, \n",
    "                           f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Top 3 ì•µì»¤ ì¶œë ¥\n",
    "            top_indices = np.argsort(anchor_vector)[-3:][::-1]\n",
    "            print(f\"\\nâ­ Top 3 Most Important Anchors:\\n\")\n",
    "            for rank, idx in enumerate(top_indices, 1):\n",
    "                if anchor_vector[idx] > 0:\n",
    "                    print(f\"  {rank}. Chunk {idx} (score: {anchor_vector[idx]:.2f})\")\n",
    "                    print(f\"     {chunks[idx][:100]}...\" if len(chunks[idx]) > 100 else f\"     {chunks[idx]}\")\n",
    "                    print()\n",
    "        else:\n",
    "            print(\"âš ï¸  Anchor scores are all zero or only one chunk.\")\n",
    "\n",
    "# ì˜ˆì œ ì„ íƒ (example_1.json ë¶„ì„)\n",
    "if result_files:\n",
    "    # ê°€ì¥ í° íŒŒì¼ (ì„±ê³µí•œ ì˜ˆì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ)\n",
    "    largest_file = max(result_files, key=lambda f: f.stat().st_size)\n",
    "    print(f\"\\në¶„ì„í•  íŒŒì¼: {largest_file.name}\\n\")\n",
    "    analyze_example(largest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ì „ì²´ í†µê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  ê²°ê³¼ íŒŒì¼ í†µê³„\n",
    "stats = {\n",
    "    'total_files': 0,\n",
    "    'successful': 0,\n",
    "    'errors': 0,\n",
    "    'total_qa_pairs': 0,\n",
    "    'all_anchor_scores': [],\n",
    "    'chunks_counts': []\n",
    "}\n",
    "\n",
    "for json_file in result_files:\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    stats['total_files'] += 1\n",
    "    \n",
    "    if 'error' in data:\n",
    "        stats['errors'] += 1\n",
    "    elif data.get('successful_pairs', 0) > 0:\n",
    "        stats['successful'] += 1\n",
    "        stats['total_qa_pairs'] += data['successful_pairs']\n",
    "        \n",
    "        for qa in data.get('qa_pairs', []):\n",
    "            anchor_vec = qa.get('anchor_vector', [])\n",
    "            if anchor_vec and len(anchor_vec) > 0:\n",
    "                stats['all_anchor_scores'].extend(anchor_vec)\n",
    "                stats['chunks_counts'].append(len(qa.get('chunks', [])))\n",
    "\n",
    "print(f\"ğŸ“ˆ ì „ì²´ í†µê³„:\")\n",
    "print(f\"  - ì´ íŒŒì¼: {stats['total_files']}\")\n",
    "print(f\"  - ì„±ê³µ: {stats['successful']}\")\n",
    "print(f\"  - ì—ëŸ¬: {stats['errors']}\")\n",
    "print(f\"  - ì´ QA pairs: {stats['total_qa_pairs']}\")\n",
    "\n",
    "if stats['all_anchor_scores']:\n",
    "    anchor_scores = np.array(stats['all_anchor_scores'])\n",
    "    non_zero = anchor_scores[anchor_scores > 0]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Anchor Score ë¶„í¬:\")\n",
    "    print(f\"  - ì´ ì ìˆ˜ ê°œìˆ˜: {len(anchor_scores)}\")\n",
    "    print(f\"  - Non-zero ì ìˆ˜: {len(non_zero)}\")\n",
    "    if len(non_zero) > 0:\n",
    "        print(f\"  - í‰ê· : {non_zero.mean():.2f}\")\n",
    "        print(f\"  - ìµœì†Œ: {non_zero.min():.2f}\")\n",
    "        print(f\"  - ìµœëŒ€: {non_zero.max():.2f}\")\n",
    "        print(f\"  - í‘œì¤€í¸ì°¨: {non_zero.std():.2f}\")\n",
    "    \n",
    "    # Anchor Score ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "    if len(non_zero) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(non_zero, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        plt.axvline(non_zero.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {non_zero.mean():.2f}')\n",
    "        plt.xlabel('Anchor Score', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.title('Distribution of Non-Zero Anchor Scores', fontsize=14, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if stats['chunks_counts']:\n",
    "    print(f\"\\nğŸ“ Chunks í†µê³„:\")\n",
    "    print(f\"  - í‰ê·  chunks ìˆ˜: {np.mean(stats['chunks_counts']):.1f}\")\n",
    "    print(f\"  - ìµœì†Œ chunks: {min(stats['chunks_counts'])}\")\n",
    "    print(f\"  - ìµœëŒ€ chunks: {max(stats['chunks_counts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” íŠ¹ì • ì˜ˆì œ ì„ íƒí•´ì„œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›í•˜ëŠ” ì˜ˆì œ ë²ˆí˜¸ ì„ íƒ (0, 1, 2, ...)\n",
    "example_number = 1\n",
    "\n",
    "example_file = output_dir / f'example_{example_number}.json'\n",
    "if example_file.exists():\n",
    "    analyze_example(example_file)\n",
    "else:\n",
    "    print(f\"âŒ {example_file.name} not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ ëª¨ë“  ì˜ˆì œ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  ì„±ê³µí•œ ì˜ˆì œì˜ í‰ê·  anchor score ë¹„êµ\n",
    "example_stats = []\n",
    "\n",
    "for json_file in result_files:\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'error' not in data and data.get('successful_pairs', 0) > 0:\n",
    "        for qa_idx, qa in enumerate(data.get('qa_pairs', [])):\n",
    "            anchor_vec = np.array(qa.get('anchor_vector', []))\n",
    "            if len(anchor_vec) > 0:\n",
    "                non_zero = anchor_vec[anchor_vec > 0]\n",
    "                if len(non_zero) > 0:\n",
    "                    example_stats.append({\n",
    "                        'name': f\"{json_file.stem}_qa{qa_idx}\",\n",
    "                        'max_score': anchor_vec.max(),\n",
    "                        'mean_score': non_zero.mean(),\n",
    "                        'num_chunks': len(anchor_vec)\n",
    "                    })\n",
    "\n",
    "if example_stats:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    names = [s['name'] for s in example_stats]\n",
    "    max_scores = [s['max_score'] for s in example_stats]\n",
    "    mean_scores = [s['mean_score'] for s in example_stats]\n",
    "    \n",
    "    # Max scores\n",
    "    ax1.bar(range(len(names)), max_scores, color='coral', alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Example', fontsize=12)\n",
    "    ax1.set_ylabel('Max Anchor Score', fontsize=12)\n",
    "    ax1.set_title('Maximum Anchor Score per Example', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(range(len(names)))\n",
    "    ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Mean scores\n",
    "    ax2.bar(range(len(names)), mean_scores, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Example', fontsize=12)\n",
    "    ax2.set_ylabel('Mean Anchor Score', fontsize=12)\n",
    "    ax2.set_title('Mean Anchor Score per Example', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(range(len(names)))\n",
    "    ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  No successful examples with anchor scores found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­:\\n\")\n",
    "print(\"1. âœ… Method 3 (Attention Suppression)ì´ ì„±ê³µì ìœ¼ë¡œ ì‘ë™\")\n",
    "print(\"   - KL divergence ê³„ì‚° ì •ìƒ\")\n",
    "print(\"   - Thought anchor ì‹ë³„ ê°€ëŠ¥\\n\")\n",
    "\n",
    "print(\"2. ğŸ“Š ì¤‘ìš”í•œ ì•µì»¤ëŠ” ì£¼ë¡œ:\")\n",
    "print(\"   - ê³„ì‚° ê³¼ì •ì´ í¬í•¨ëœ ë¬¸ì¥\")\n",
    "print(\"   - ìµœì¢… ë‹µë³€ ë¬¸ì¥\")\n",
    "print(\"   - êµ¬ì²´ì ì¸ ìˆ˜ì¹˜/ë…¼ë¦¬ê°€ ìˆëŠ” ë¬¸ì¥\\n\")\n",
    "\n",
    "print(\"3. âš ï¸  ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„:\")\n",
    "print(\"   - ì¼ë¶€ ì˜ˆì œëŠ” ì´ë¯¸ì§€ ê²½ë¡œ ë¬¸ì œë¡œ ìŠ¤í‚µë¨\")\n",
    "print(\"   - ë„ˆë¬´ ì§§ì€ reasoningì€ chunk ë¶„í•  ì–´ë ¤ì›€\")\n",
    "print(\"   - ë” ë§ì€ chartqa ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ í•„ìš”\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thoughtanchors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
