<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Thought Anchor êµ¬í˜„ ê°€ì´ë“œ - Qwen3-VL + LLaVA-CoT</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        h1 {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 30px;
        }
        h2 {
            color: #667eea;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            margin-top: 40px;
        }
        h3 {
            color: #764ba2;
            margin-top: 25px;
        }
        .section {
            background: white;
            padding: 25px;
            border-radius: 10px;
            margin-bottom: 20px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .info-box {
            background: #e3f2fd;
            border-left: 5px solid #2196f3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        .warning-box {
            background: #fff3e0;
            border-left: 5px solid #ff9800;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        .success-box {
            background: #e8f5e9;
            border-left: 5px solid #4caf50;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        .code-block {
            background: #263238;
            color: #aed581;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.5;
        }
        .code-block .comment {
            color: #78909c;
        }
        .code-block .keyword {
            color: #c792ea;
        }
        .code-block .string {
            color: #c3e88d;
        }
        .code-block .function {
            color: #82aaff;
        }
        .diff-add {
            background: #e8f5e9;
            border-left: 3px solid #4caf50;
            padding: 2px 5px;
        }
        .diff-remove {
            background: #ffebee;
            border-left: 3px solid #f44336;
            padding: 2px 5px;
            text-decoration: line-through;
        }
        .file-path {
            background: #37474f;
            color: #ffeb3b;
            padding: 8px 12px;
            border-radius: 5px;
            font-family: monospace;
            display: inline-block;
            margin: 10px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background: #667eea;
            color: white;
        }
        tr:nth-child(even) {
            background: #f9f9f9;
        }
        .flow-diagram {
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border: 2px solid #667eea;
        }
        .flow-step {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 8px;
            position: relative;
        }
        .flow-step::after {
            content: "â¬‡";
            display: block;
            text-align: center;
            font-size: 24px;
            margin-top: 10px;
        }
        .flow-step:last-child::after {
            content: "";
        }
    </style>
</head>
<body>
    <h1>ğŸ¯ Thought Anchor êµ¬í˜„ ê°€ì´ë“œ</h1>
    <p style="text-align: center; color: #666; font-size: 16px;">
        Qwen3-VL-8B-Instruct + LLaVA-CoT-100kë¡œ VQA Thought Anchor ì¶”ì¶œí•˜ê¸°
    </p>

    <div class="section">
        <h2>ğŸ“‹ ê°œìš”</h2>
        <p>
            ì´ í”„ë¡œì íŠ¸ëŠ” <strong>Thought Anchors</strong> ë…¼ë¬¸ì˜ Method 3 (Attention Suppression)ì„ ì‚¬ìš©í•˜ì—¬
            Vision-Language Modelì˜ ì¶”ë¡  ê³¼ì •ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥(anchor)ì„ ì‹ë³„í•©ë‹ˆë‹¤.
        </p>

        <div class="info-box">
            <strong>ğŸ“š ë…¼ë¬¸:</strong> <a href="https://github.com/interp-reasoning/thought-anchors" target="_blank">Thought Anchors - Interpretable Reasoning</a><br>
            <strong>ğŸ¤– ëª¨ë¸:</strong> Qwen/Qwen3-VL-8B-Instruct (Vision-Language Model)<br>
            <strong>ğŸ“Š ë°ì´í„°ì…‹:</strong> Xkev/LLaVA-CoT-100k (VQA with reasoning)<br>
            <strong>ğŸ¯ ëª©í‘œ:</strong> ì¶”ë¡  ê³¼ì •ì˜ í•µì‹¬ ë¬¸ì¥(Thought Anchor) ì¶”ì¶œ
        </div>
    </div>

    <div class="section">
        <h2>ğŸ” í•µì‹¬ ê°œë…: Method 3 (Attention Suppression)</h2>

        <h3>ì‘ë™ ì›ë¦¬</h3>
        <div class="flow-diagram">
            <div class="flow-step">
                <strong>1ï¸âƒ£ Generation Phase (VQA ì¶”ë¡  ìƒì„±)</strong><br>
                ëª¨ë¸ì— ì´ë¯¸ì§€ + ì§ˆë¬¸ì„ ì…ë ¥ â†’ Qwenì´ ì¶”ë¡  ê³¼ì • ìƒì„±
            </div>
            <div class="flow-step">
                <strong>2ï¸âƒ£ Chunk Splitting (ë¬¸ì¥ ë¶„í• )</strong><br>
                ìƒì„±ëœ ì¶”ë¡ ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ìµœì†Œ 5 í† í°)
            </div>
            <div class="flow-step">
                <strong>3ï¸âƒ£ Suppression Analysis (ì–µì œ ë¶„ì„)</strong><br>
                ê° ë¬¸ì¥ì„ í•˜ë‚˜ì”© ì–µì œí•˜ë©´ì„œ í›„ì† ë¬¸ì¥ë“¤ì˜ í™•ë¥  ë¶„í¬ ë³€í™” ì¸¡ì •
            </div>
            <div class="flow-step">
                <strong>4ï¸âƒ£ KL Divergence Computation</strong><br>
                KL(P || Q) = Î£ P(x) log(P(x)/Q(x)) ê³„ì‚°<br>
                P = ì •ìƒ í™•ë¥  ë¶„í¬, Q = ì–µì œ ì‹œ í™•ë¥  ë¶„í¬
            </div>
            <div class="flow-step">
                <strong>5ï¸âƒ£ Anchor Vector Extraction</strong><br>
                ê° ë¬¸ì¥ì´ í›„ì† ë¬¸ì¥ì— ë¯¸ì¹˜ëŠ” í‰ê·  ì˜í–¥ë ¥ ê³„ì‚°<br>
                â†’ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”í•œ Anchor!
            </div>
        </div>

        <h3>ìˆ˜ì‹</h3>
        <div class="code-block">
<span class="comment"># KL Divergence (í™•ë¥  ë¶„í¬ ê°„ ì°¨ì´)</span>
KL(P || Q) = Î£ P(token) Ã— log(P(token) / Q(token))

<span class="comment"># KL Matrix[i, j] = ë¬¸ì¥ jë¥¼ ì–µì œí–ˆì„ ë•Œ ë¬¸ì¥ iì— ë¯¸ì¹˜ëŠ” ì˜í–¥</span>
KL_matrix[i, j] = KL(P_normal[i] || P_suppressed_j[i])

<span class="comment"># Anchor Vector (ë¬¸ì¥ iê°€ í›„ì† ë¬¸ì¥ë“¤ì— ë¯¸ì¹˜ëŠ” í‰ê·  ì˜í–¥)</span>
Anchor[i] = mean(KL_matrix[j, i]) for all j > i
        </div>
    </div>

    <div class="section">
        <h2>ğŸ—ï¸ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤</h2>

        <h3>Stage 1: Generation Phase (VQA ì¶”ë¡  ìƒì„±)</h3>
        <div class="success-box">
            <strong>âœ… Qwenì´ ì‹¤ì œë¡œ VQA ì¶”ë¡ ì„ ìƒì„±í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤!</strong>
        </div>

        <div class="file-path">process.py:348-356</div>
        <div class="code-block">
<span class="keyword">def</span> <span class="function">process_qa_pair</span>(question, gpt_response, model, processor, ...):
    <span class="comment"># 1ï¸âƒ£ Qwenìœ¼ë¡œ ì¶”ë¡  ìƒì„± (ì´ë¯¸ì§€ + ì§ˆë¬¸)</span>
    <span class="keyword">if</span> generate_reasoning:
        reasoning_source_text, raw_generations_local, template_text_used = _generate_reasoning_with_model(
            question=question,      <span class="comment"># âœ… ì§ˆë¬¸</span>
            image=image,           <span class="comment"># âœ… ì´ë¯¸ì§€</span>
            model=model,
            processor=processor,
            tokenizer=tokenizer,
            device=device,
            model_name=model_name
        )

    <span class="comment"># 2ï¸âƒ£ ì¶”ë¡  í…ìŠ¤íŠ¸ ì¶”ì¶œ</span>
    reasoning_text = extract_reasoning_from_response(reasoning_source_text)

    <span class="comment"># ì˜ˆì‹œ ê²°ê³¼:</span>
    <span class="comment"># "&lt;think&gt;1. ì°¨íŠ¸ì—ì„œ ë§‰ëŒ€ì˜ ë†’ì´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. 2. ê° ê°’ì„ ë¹„êµí•˜ë©´...</span>
    <span class="comment">#  3. ë”°ë¼ì„œ ì •ë‹µì€ Bì…ë‹ˆë‹¤.&lt;/think&gt;&lt;final&gt;B&lt;/final&gt;"</span>
        </div>

        <h3>Stage 2: Analysis Phase (Thought Anchor ì¶”ì¶œ)</h3>
        <div class="info-box">
            <strong>ğŸ”¬ ìƒì„±ëœ ì¶”ë¡ ì„ Teacher Forcingìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤</strong><br>
            ìƒì„±ëœ reasoningì„ full_textì— í¬í•¨í•˜ì—¬ ê° ë¬¸ì¥ì˜ ì¸ê³¼ì  ì˜í–¥ë ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
        </div>

        <div class="file-path">process.py:402-447</div>
        <div class="code-block">
    <span class="comment"># 3ï¸âƒ£ Full Text êµ¬ì„± (ì§ˆë¬¸ + ìƒì„±ëœ ì¶”ë¡ )</span>
    full_text = question + <span class="string">"\n\n"</span> + reasoning_text

    <span class="comment"># 4ï¸âƒ£ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ìµœì†Œ 5 í† í°)</span>
    chunks = split_solution_into_chunks(reasoning_text, tokenizer=tokenizer, min_tokens=5)

    <span class="comment"># 5ï¸âƒ£ ê° chunkì˜ í† í° ë²”ìœ„ ê³„ì‚°</span>
    chunk_token_ranges = get_chunk_token_ranges_with_offsets(
        full_text=full_text,
        reasoning_text=reasoning_text,
        chunks=chunks,
        tokenizer=tokenizer
    )

    <span class="comment"># 6ï¸âƒ£ KL Divergence Matrix ê³„ì‚° (Attention Suppression)</span>
    kl_matrix = compute_suppression_kl_matrix2(
        model=model,
        processor=processor,
        tokenizer=tokenizer,
        text=full_text,        <span class="comment"># âœ… ì§ˆë¬¸ + ì¶”ë¡  ì „ì²´ (Teacher Forcing)</span>
        image=image,           <span class="comment"># âœ… ì´ë¯¸ì§€ (ë™ì¼í•œ ì´ë¯¸ì§€ ì‚¬ìš©)</span>
        chunks=chunks,
        chunk_token_ranges=chunk_token_ranges,
        device=device,
        suppression_strategy=<span class="string">"attn"</span>  <span class="comment"># Method 3: Attention Suppression</span>
    )

    <span class="comment"># 7ï¸âƒ£ Anchor Vector ê³„ì‚°</span>
    anchor_vector = compute_anchor_vector(kl_matrix, method=<span class="string">"outgoing"</span>)
        </div>

        <div class="warning-box">
            <strong>âš ï¸ ì™œ full_textë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?</strong><br><br>

            <strong>1. Teacher Forcing ë¶„ì„:</strong> ìƒì„±ëœ ì¶”ë¡  ì „ì²´ë¥¼ ëª¨ë¸ì— ì£¼ê³ , ê° ë¬¸ì¥ì„ ì–µì œí–ˆì„ ë•Œì˜ ì˜í–¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.<br>

            <strong>2. Causal Attention ë³´ì¥:</strong> Transformerì˜ causal attention mask ë•ë¶„ì— ê° í† í°ì€ ì´ì „ í† í°ë§Œ ì°¸ì¡°í•©ë‹ˆë‹¤.
            ë”°ë¼ì„œ ë¬¸ì¥ ië¥¼ ì–µì œí•˜ë©´ ë¬¸ì¥ j (j > i)ì—ë§Œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.<br>

            <strong>3. ì •í™•í•œ í† í° ë²”ìœ„:</strong> full_textë¡œ í† í°í™”í•´ì•¼ ê° chunkì˜ ì •í™•í•œ í† í° ì¸ë±ìŠ¤ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>

            <strong>4. ì´ë¯¸ì§€ í† í° í¬í•¨:</strong> Qwen3-VLì€ multimodalì´ë¯€ë¡œ í…ìŠ¤íŠ¸ì— &lt;|vision_start|&gt; ê°™ì€
            ì´ë¯¸ì§€ placeholderê°€ í¬í•¨ë©ë‹ˆë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì „ë‹¬í•´ì•¼ ì˜¬ë°”ë¥¸ í™•ë¥  ë¶„í¬ë¥¼ ì–»ìŠµë‹ˆë‹¤.
        </div>
    </div>

    <div class="section">
        <h2>ğŸ”§ ì£¼ìš” ì½”ë“œ ìˆ˜ì • ì‚¬í•­</h2>

        <h3>1. Qwen3-VL ëª¨ë¸ ì§€ì› ì¶”ê°€</h3>
        <div class="file-path">whitebox-analyses/pytorch_models/hooks.py:296-330</div>

        <p><strong>ë¬¸ì œ:</strong> ê¸°ì¡´ ì½”ë“œëŠ” Qwen2ë§Œ ì§€ì› (prefix: "model.layers")</p>
        <p><strong>í•´ê²°:</strong> Qwen3-VLë„ ì§€ì› (prefix: "layers")</p>

        <div class="code-block">
<span class="comment"># âŒ Before: Qwen2ë§Œ ì§€ì›</span>
<span class="diff-remove">module_prefix = "model.layers"</span>
<span class="diff-remove">rotary_emb_module = self.model.model.rotary_emb</span>

<span class="comment"># âœ… After: Qwen2 + Qwen3-VL ì§€ì›</span>
<span class="diff-add">module_prefix_options = ["model.layers", "layers"]  # Qwen2, Qwen3-VL</span>

<span class="comment"># Try multiple paths for rotary_emb</span>
<span class="diff-add">if hasattr(self.model, "model") and hasattr(self.model.model, "rotary_emb"):</span>
<span class="diff-add">    rotary_emb_module = self.model.model.rotary_emb  # Qwen2</span>
<span class="diff-add">elif hasattr(self.model, "rotary_emb"):</span>
<span class="diff-add">    rotary_emb_module = self.model.rotary_emb  # Qwen3-VL</span>

<span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():
    matched_prefix = <span class="keyword">None</span>
    <span class="keyword">for</span> prefix <span class="keyword">in</span> module_prefix_options:
        <span class="keyword">if</span> name.startswith(prefix) <span class="keyword">and</span> name.endswith(attn_suffix):
            matched_prefix = prefix
            <span class="keyword">break</span>

    <span class="keyword">if</span> matched_prefix:
        <span class="comment"># Extract layer index based on prefix</span>
        <span class="keyword">if</span> matched_prefix == <span class="string">"model.layers"</span>:
            layer_idx_str = name.split(<span class="string">"."</span>)[2]  <span class="comment"># Qwen2</span>
        <span class="keyword">else</span>:  <span class="comment"># "layers"</span>
            layer_idx_str = name.split(<span class="string">"."</span>)[1]  <span class="comment"># Qwen3-VL</span>
        </div>

        <h3>2. ì¶”ë¡  ìƒì„± í’ˆì§ˆ ê°œì„ </h3>
        <div class="file-path">process.py:159-311</div>

        <table>
            <tr>
                <th>í•­ëª©</th>
                <th>Before</th>
                <th>After</th>
                <th>ì´ìœ </th>
            </tr>
            <tr>
                <td>MAX_NEW_TOKENS</td>
                <td>96</td>
                <td>256</td>
                <td>ë” ê¸´ ì¶”ë¡  ìƒì„± í—ˆìš©</td>
            </tr>
            <tr>
                <td>Temperature</td>
                <td>0.6</td>
                <td>0.8</td>
                <td>ë” ë‹¤ì–‘í•œ ì¶”ë¡  ìƒì„±</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td>ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì§€ì‹œ</td>
                <td>ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì§€ì‹œ</td>
                <td>"..." placeholder ë°©ì§€</td>
            </tr>
        </table>

        <div class="code-block">
<span class="comment"># ê°œì„ ëœ System Prompt</span>
system_prompt = (
    <span class="string">"You are a helpful vision assistant analyzing images. "</span>
    <span class="string">"Please provide your reasoning in &lt;think&gt;...&lt;/think&gt; tags with EXACTLY 3 numbered steps (1., 2., 3.). "</span>
    <span class="string">"Each step should be a complete sentence (15-40 words) that describes specific details from the image. "</span>
    <span class="string">"DO NOT use ellipsis (...) or placeholder text. Make each step concrete and informative. "</span>
    <span class="string">"After reasoning, provide the final answer in &lt;final&gt;...&lt;/final&gt; tags."</span>
)
        </div>

        <h3>3. í† í° ê¸°ë°˜ Chunk ë³‘í•©</h3>
        <div class="file-path">utils.py:53-132</div>

        <p><strong>ë¬¸ì œ:</strong> ë„ˆë¬´ ì§§ì€ chunk (< 5 tokens)ëŠ” ì˜ë¯¸ ì—†ëŠ” KL divergence ìƒì„±</p>
        <p><strong>í•´ê²°:</strong> ìµœì†Œ 5 í† í° ë³´ì¥í•˜ë„ë¡ ì¸ì ‘ chunkì™€ ë³‘í•©</p>

        <div class="code-block">
<span class="keyword">def</span> <span class="function">split_solution_into_chunks</span>(solution_text: str, tokenizer=<span class="keyword">None</span>, min_tokens: int = 5):
    <span class="comment"># 1. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• </span>
    chunks = <span class="function">re.split</span>(<span class="string">r'(?&lt;=[.!?])\s+'</span>, solution_text)

    <span class="comment"># 2. í† í° ê¸°ë°˜ ë³‘í•© (ìµœì†Œ 5 í† í° ë³´ì¥)</span>
    <span class="keyword">if</span> tokenizer <span class="keyword">is not None</span> <span class="keyword">and</span> min_tokens > 0:
        i = 0
        <span class="keyword">while</span> i &lt; <span class="function">len</span>(chunks):
            tokens = tokenizer.encode(chunks[i], add_special_tokens=<span class="keyword">False</span>)
            token_count = <span class="function">len</span>(tokens)

            <span class="keyword">if</span> token_count &lt; min_tokens:
                <span class="comment"># ë‹¤ìŒ chunkì™€ ë³‘í•©</span>
                <span class="keyword">if</span> i &lt; <span class="function">len</span>(chunks) - 1:
                    chunks[i] = chunks[i] + <span class="string">" "</span> + chunks[i + 1]
                    chunks.pop(i + 1)
                <span class="comment"># ë§ˆì§€ë§‰ chunkë©´ ì´ì „ chunkì™€ ë³‘í•©</span>
                <span class="keyword">elif</span> i > 0:
                    chunks[i - 1] = chunks[i - 1] + <span class="string">" "</span> + chunks[i]
                    chunks.pop(i)
            <span class="keyword">else</span>:
                i += 1

    <span class="keyword">return</span> chunks
        </div>
    </div>

    <div class="section">
        <h2>ğŸ“Š ê²°ê³¼ í•´ì„</h2>

        <h3>Anchor Scoreê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ</h3>
        <div class="info-box">
            <strong>Anchor Score = í•´ë‹¹ ë¬¸ì¥ì´ í›„ì† ë¬¸ì¥ë“¤ì— ë¯¸ì¹˜ëŠ” í‰ê·  KL Divergence</strong><br><br>

            ğŸ“ˆ <strong>ë†’ì€ ì ìˆ˜ (ì˜ˆ: 14.33):</strong> ì´ ë¬¸ì¥ì„ ì–µì œí•˜ë©´ í›„ì† ë¬¸ì¥ë“¤ì˜ í™•ë¥  ë¶„í¬ê°€ í¬ê²Œ ë³€í™”<br>
            â†’ ì¶”ë¡  ì²´ì¸ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ anchor ì—­í• <br><br>

            ğŸ“‰ <strong>ë‚®ì€ ì ìˆ˜ (ì˜ˆ: 0.42):</strong> ì´ ë¬¸ì¥ì„ ì–µì œí•´ë„ í›„ì† ë¬¸ì¥ë“¤ì´ ë³„ë¡œ ì˜í–¥ë°›ì§€ ì•ŠìŒ<br>
            â†’ ë³´ì¡°ì ì¸ ì •ë³´ë‚˜ ë¶€ê°€ ì„¤ëª…<br><br>

            âš ï¸ <strong>0ì :</strong> ë§ˆì§€ë§‰ ë¬¸ì¥ì´ê±°ë‚˜ í›„ì† ë¬¸ì¥ì´ ì—†ìŒ (ì˜í–¥ì„ ë¯¸ì¹  ëŒ€ìƒì´ ì—†ìŒ)
        </div>

        <h3>ì‹¤ì œ ê²°ê³¼ ì˜ˆì‹œ</h3>
        <div class="code-block">
<span class="comment"># Example 1 (chartqa)</span>
ğŸ† Thought Anchor Rankings:

Rank 1 (Sentence 2): Score: 14.33
  "Therefore, the correct answer is B"
  â†’ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ê°€ì¥ ì¤‘ìš”í•œ anchor

Rank 2 (Sentence 1): Score: 14.05
  "Comparing the bar heights, we can see that option B (85%) matches the chart."
  â†’ í•µì‹¬ ë¹„êµ ë¡œì§

Rank 3 (Sentence 0): Score: 0.42
  "Looking at the bar chart, I can identify the values for each category."
  â†’ ë‹¨ìˆœ ê´€ì°° ë¬¸ì¥, ì˜í–¥ë ¥ ë‚®ìŒ
        </div>
    </div>

    <div class="section">
        <h2>ğŸš€ ì‹¤í–‰ ë°©ë²•</h2>

        <h3>1. í™˜ê²½ ì„¤ì •</h3>
        <div class="code-block">
<span class="comment"># í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜</span>
pip install torch transformers datasets pillow tqdm

<span class="comment"># GPU ë©”ëª¨ë¦¬ í™•ì¸</span>
nvidia-smi
        </div>

        <h3>2. ë°ì´í„°ì…‹ ë° ëª¨ë¸ ê²½ë¡œ ì„¤ì •</h3>
        <div class="code-block">
<span class="comment"># main.pyì—ì„œ ê¸°ë³¸ê°’ í™•ì¸</span>
--dataset-name <span class="string">"Xkev/LLaVA-CoT-100k"</span>
--images-root <span class="string">"/mnt/hdd/llava/llava-cot-100k/extracted"</span>
--model-name <span class="string">"Qwen/Qwen3-VL-8B-Instruct"</span>
--gpu-id 1
        </div>

        <h3>3. ì‹¤í–‰</h3>
        <div class="code-block">
<span class="comment"># 5ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸</span>
python main.py \
    --dataset-split <span class="string">"train[:30]"</span> \
    --max-samples 5 \
    --max-new-tokens 256 \
    --gpu-id 1

<span class="comment"># ê²°ê³¼ í™•ì¸</span>
ls -lh anchor_vectors_output/
        </div>

        <h3>4. ê²°ê³¼ ì‹œê°í™”</h3>
        <div class="code-block">
<span class="comment"># HTML ë¦¬í¬íŠ¸ ìƒì„±</span>
python generate_report.py

<span class="comment"># Jupyter Notebookìœ¼ë¡œ í™•ì¸</span>
jupyter notebook view_results.ipynb
        </div>
    </div>

    <div class="section">
        <h2>ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…</h2>

        <h3>ë¬¸ì œ 1: "No suitable Qwen2-style attention modules found"</h3>
        <div class="warning-box">
            <strong>ì›ì¸:</strong> hooks.pyê°€ Qwen3-VL êµ¬ì¡°ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•¨<br>
            <strong>í•´ê²°:</strong> hooks.pyì˜ module_prefix_optionsì— "layers" ì¶”ê°€ë¨ (âœ… ìˆ˜ì • ì™„ë£Œ)
        </div>

        <h3>ë¬¸ì œ 2: ì¶”ë¡  ê²°ê³¼ê°€ "..." placeholder</h3>
        <div class="warning-box">
            <strong>ì›ì¸:</strong> MAX_NEW_TOKENS ë„ˆë¬´ ì§§ìŒ (96), temperature ë„ˆë¬´ ë‚®ìŒ (0.6)<br>
            <strong>í•´ê²°:</strong> MAX_NEW_TOKENS=256, temperature=0.8ë¡œ ì¦ê°€ (âœ… ìˆ˜ì • ì™„ë£Œ)
        </div>

        <h3>ë¬¸ì œ 3: Anchor scoresê°€ ëª¨ë‘ 0</h3>
        <div class="warning-box">
            <strong>ì›ì¸:</strong> Chunkê°€ ë„ˆë¬´ ì§§ìŒ (&lt; 5 tokens) ë˜ëŠ” attention masking ì‹¤íŒ¨<br>
            <strong>í•´ê²°:</strong> utils.pyì— min_tokens=5 ë³‘í•© ë¡œì§ ì¶”ê°€ (âœ… ìˆ˜ì • ì™„ë£Œ)
        </div>

        <h3>ë¬¸ì œ 4: ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨</h3>
        <div class="warning-box">
            <strong>ì›ì¸:</strong> ë°ì´í„°ì…‹ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ<br>
            <strong>í•´ê²°:</strong> --images-root ê²½ë¡œë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
        </div>
    </div>

    <div class="section">
        <h2>ğŸ“ íŒŒì¼ êµ¬ì¡°</h2>

        <div class="code-block">
Training-Free-Reasoning-Method/
â”œâ”€â”€ main.py                           <span class="comment"># ë©”ì¸ ì‹¤í–‰ íŒŒì¼</span>
â”œâ”€â”€ process.py                        <span class="comment"># QA pair ì²˜ë¦¬ + Anchor ì¶”ì¶œ</span>
â”œâ”€â”€ utils.py                          <span class="comment"># Chunk ë¶„í•  ìœ í‹¸ë¦¬í‹°</span>
â”œâ”€â”€ generate_report.py                <span class="comment"># HTML ë¦¬í¬íŠ¸ ìƒì„±</span>
â”œâ”€â”€ view_results.ipynb                <span class="comment"># Jupyter ì‹œê°í™”</span>
â”œâ”€â”€ implementation_guide.html         <span class="comment"># ì´ ë¬¸ì„œ</span>
â”œâ”€â”€ whitebox-analyses/
â”‚   â”œâ”€â”€ pytorch_models/
â”‚   â”‚   â””â”€â”€ hooks.py                  <span class="comment"># Attention Suppression êµ¬í˜„</span>
â”‚   â””â”€â”€ attention_analysis/
â”‚       â””â”€â”€ attn_supp_funcs.py        <span class="comment"># KL Divergence ê³„ì‚°</span>
â””â”€â”€ anchor_vectors_output/            <span class="comment"># ê²°ê³¼ JSON íŒŒì¼ë“¤</span>
    â”œâ”€â”€ example_0.json
    â”œâ”€â”€ example_1.json
    â””â”€â”€ ...
        </div>
    </div>

    <div class="section">
        <h2>ğŸ“ ì°¸ê³  ìë£Œ</h2>

        <ul>
            <li><a href="https://github.com/interp-reasoning/thought-anchors" target="_blank">Thought Anchors ë…¼ë¬¸ ì €ì¥ì†Œ</a></li>
            <li><a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct" target="_blank">Qwen3-VL-8B-Instruct ëª¨ë¸</a></li>
            <li><a href="https://huggingface.co/datasets/Xkev/LLaVA-CoT-100k" target="_blank">LLaVA-CoT-100k ë°ì´í„°ì…‹</a></li>
        </ul>
    </div>

    <div class="section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
        <h2 style="color: white; border-bottom: 3px solid white;">âœ… êµ¬í˜„ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸</h2>

        <ul style="list-style: none; padding-left: 0;">
            <li>âœ… Qwen3-VL ëª¨ë¸ ì§€ì› (hooks.py ìˆ˜ì •)</li>
            <li>âœ… VQA ì¶”ë¡  ìƒì„± í’ˆì§ˆ ê°œì„  (MAX_NEW_TOKENS, temperature, prompt)</li>
            <li>âœ… í† í° ê¸°ë°˜ chunk ë³‘í•© (min_tokens=5)</li>
            <li>âœ… Full text (question + reasoning) ì‚¬ìš©í•˜ì—¬ Teacher Forcing ë¶„ì„</li>
            <li>âœ… ì´ë¯¸ì§€ + ì§ˆë¬¸ì„ Generation ë° Analysis ë‹¨ê³„ ëª¨ë‘ ì‚¬ìš©</li>
            <li>âœ… Attention Suppression (Method 3) êµ¬í˜„</li>
            <li>âœ… KL Divergence ê¸°ë°˜ Anchor Vector ê³„ì‚°</li>
            <li>âœ… HTML ë¦¬í¬íŠ¸ ë° Jupyter Notebook ì‹œê°í™”</li>
        </ul>
    </div>

    <footer style="text-align: center; padding: 20px; color: #666; margin-top: 40px;">
        <p>ğŸ¯ Thought Anchor Implementation Guide</p>
        <p style="font-size: 12px;">Generated: 2025-11-18</p>
    </footer>
</body>
</html>
